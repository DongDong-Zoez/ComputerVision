{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColorNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNVhcqgBM89cw39u2YD7RYP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DongDong-Zoez/ComputerVision/blob/main/Image%20Segmentation/UNet/ColorNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train UNet from scratch"
      ],
      "metadata": {
        "id": "kdTPkvn7Oq4W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEvJuDd3OeJg"
      },
      "outputs": [],
      "source": [
        "# connect to your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the image to training and validation"
      ],
      "metadata": {
        "id": "9XB_yIuPOu4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import splitfolders\n",
        "#splitfolders.ratio(input='/content/gdrive/MyDrive/AnimeFace/', output='/content/gdrive/MyDrive/AnimeFace/split', seed=1337, ratio=(0.8, 0.2))"
      ],
      "metadata": {
        "id": "0FhcvObYOl1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Architecture\n",
        "\n",
        "TO DO:\n",
        "\n",
        "1. In encoder part, use Conv2d replace MaxPool2d, and see how it works.\n",
        "2. In decoder part, use ConvTranpose2d replace Upsample, and see how it works.\n",
        "\n",
        "NOTE:\n",
        "\n",
        "1. in_channels = 3 for RGB images\n",
        "2. change out_channels to your custom setting"
      ],
      "metadata": {
        "id": "cL7_5BNjO1hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class DownSampleLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(DownSampleLayer, self).__init__()\n",
        "        \n",
        "        self.DoubleConv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=out_ch),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=out_ch),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(num_features=out_ch),\n",
        "            nn.ReLU(),\n",
        "            #nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.DoubleConv(x)\n",
        "        d = self.downsample(x)\n",
        "        \n",
        "        return x, d\n",
        "    \n",
        "class UpSampleLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(UpSampleLayer, self).__init__()\n",
        "        \n",
        "        self.DoubleConv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch, out_channels=out_ch*2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=out_ch*2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(in_channels=out_ch*2, out_channels=out_ch*2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=out_ch*2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.Upsample = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=out_ch*2, out_channels=out_ch, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(num_features=out_ch),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, copy_crop):\n",
        "        \n",
        "        x = self.DoubleConv(x)\n",
        "        u = self.Upsample(x)\n",
        "        copy_crop = torch.cat((u, copy_crop), dim=1)\n",
        "        \n",
        "        return copy_crop\n",
        "    \n",
        "class UNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "        \n",
        "        self.channels = channels\n",
        "        in_channels = 1 #RGB\n",
        "        out_channels= [16,32,64,128,256] #image tile\n",
        "        \n",
        "        self.d1 = DownSampleLayer(in_ch=in_channels, out_ch=out_channels[0])\n",
        "        self.d2 = DownSampleLayer(in_ch=out_channels[0], out_ch=out_channels[1])\n",
        "        self.d3 = DownSampleLayer(in_ch=out_channels[1], out_ch=out_channels[2])\n",
        "        self.d4 = DownSampleLayer(in_ch=out_channels[2], out_ch=out_channels[3])\n",
        "        \n",
        "        self.u1 = UpSampleLayer(in_ch=out_channels[3], out_ch=out_channels[3])\n",
        "        self.u2 = UpSampleLayer(in_ch=out_channels[4], out_ch=out_channels[2])\n",
        "        self.u3 = UpSampleLayer(in_ch=out_channels[3], out_ch=out_channels[1])\n",
        "        self.u4 = UpSampleLayer(in_ch=out_channels[2], out_ch=out_channels[0])\n",
        "        \n",
        "        self.output = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=out_channels[1], out_channels=out_channels[0], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=out_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(in_channels=out_channels[0], out_channels=out_channels[0], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=out_channels[0]),\n",
        "            nn.ReLU(),    \n",
        "            \n",
        "            nn.Conv2d(in_channels=out_channels[0], out_channels=self.channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "            \n",
        "        c1, d1 = self.d1(x)\n",
        "        c2, d2 = self.d2(d1)\n",
        "        c3, d3 = self.d3(d2)\n",
        "        c4, d4 = self.d4(d3)\n",
        "            \n",
        "        u1 = self.u1(d4, c4)\n",
        "        u2 = self.u2(u1, c3)\n",
        "        u3 = self.u3(u2, c2)\n",
        "        u4 = self.u4(u3, c1)\n",
        "            \n",
        "        out = self.output(u4)\n",
        "            \n",
        "        return out\n",
        "    \n",
        "    def save_model(self, filename):\n",
        "        torch.save(self.state_dict(), filename)\n",
        "\n",
        "    def load_model(self, filename, cpu=False):\n",
        "        if not cpu:\n",
        "            self.load_state_dict(torch.load(filename))\n",
        "        else:\n",
        "            self.__init__(self.nbase,\n",
        "                    self.nout,\n",
        "                    self.kernel_size,\n",
        "                    self.concatenation)\n",
        "\n",
        "            self.load_state_dict(torch.load(filename,\n",
        "                                      map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "aZ0WrRlTOmFa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the Dataset\n",
        "\n",
        "Noet: We use the data from [Kaggle Anime Face](https://www.kaggle.com/soumikrakshit/anime-faces)"
      ],
      "metadata": {
        "id": "6gC-D8PEPAQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import albumentations\n",
        "\n",
        "WIDTH, HEIGHT = 64, 64\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize([HEIGHT, WIDTH]),\n",
        "    transforms.ColorJitter(contrast=0.5, hue=0.25),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                  \n",
        "])\n",
        "\n",
        "class AnimeFacesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, transforms=transform ,path='/content/gdrive/MyDrive/AnimeFace/split/train/data/'):\n",
        "        self.path = path\n",
        "        self.transforms = transforms\n",
        "        for root, dirs, files in os.walk(self.path):\n",
        "            self.imgs = [self.path + file for file in files]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(self.imgs[idx])\n",
        "        img = self.transforms(img)\n",
        "        Y = img\n",
        "        X = (Y[0,:,:] * 0.299 + Y[1,:,:] * 0.587 + Y[2,:,:] * 0.114).reshape(1, HEIGHT, WIDTH)\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def show_img(self, idx):\n",
        "        img = cv2.imread(self.imgs[idx])\n",
        "        img = self.transforms(img)\n",
        "        img = np.transpose(img, [0,2,1])\n",
        "        img = np.array(img)[::-1,...]\n",
        "\n",
        "        Y = img\n",
        "        X = (Y[0,:,:] * 0.299 + Y[1,:,:] * 0.587 + Y[2,:,:] * 0.114)\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title('Gray scale')\n",
        "        plt.axis('off')\n",
        "        plt.imshow(X.T, cmap='Greys_r')\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.title('Except Output')\n",
        "        plt.axis('off')\n",
        "        plt.imshow(Y.T)\n"
      ],
      "metadata": {
        "id": "DGGSrKKhOmRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anime = AnimeFacesDataset()\n",
        "\n",
        "idx = 10\n",
        "anime.show_img(idx)"
      ],
      "metadata": {
        "id": "_3IW7s5yPmjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "7ANaWiQjPZE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(net, device, path, loss_func, epochs=100, batch_size=32, lr=0.001):\n",
        "\n",
        "    anime = AnimeFacesDataset(path=path)\n",
        "    data_loader = torch.utils.data.DataLoader(dataset=anime, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    criterion = loss_func\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for image, label in data_loader:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            image = image.to(device=device)\n",
        "            label = label.to(device=device)\n",
        "\n",
        "            pred = net(image)\n",
        "\n",
        "            loss = criterion(pred, label)\n",
        "            print('Loss/train', loss.item())\n",
        "\n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "                torch.save(net.state_dict(), '/content/gdrive/MyDrive/best_model_n.pth')\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net = UNet(channels=3)\n",
        "net.to(device=device)\n",
        "net.load_state_dict(torch.load('/content/gdrive/MyDrive/best_model_n.pth'))\n",
        "\n",
        "path = \"/content/gdrive/MyDrive/AnimeFace/split/train/data/\"\n",
        "train(net, device, path, nn.MSELoss())"
      ],
      "metadata": {
        "id": "6hzm5ZE4PtMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference"
      ],
      "metadata": {
        "id": "LLzcJRCpPtuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net = UNet(channels=3)\n",
        "#net.load_state_dict(torch.load('/content/gdrive/MyDrive/best_model_n.pth', map_location=device))\n",
        "net.to(device=device)\n",
        "net.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    anime = AnimeFacesDataset(path='/content/gdrive/MyDrive/AnimeFace/split/val/data/')\n",
        "    data_loader = torch.utils.data.DataLoader(dataset=anime, batch_size=1, shuffle=True)\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(tqdm(data_loader), 1):\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        output = net(images)\n",
        "        output = np.array(output.data.cpu()[0]) * 255\n",
        "        images = np.array(images.data.cpu()[0]).squeeze(0) * 255\n",
        "        targets = np.array(targets.data.cpu()[0]) * 255\n",
        "\n",
        "        output = np.transpose(output, [1,2,0])\n",
        "        targets = np.transpose(targets, [1,2,0])\n",
        "\n",
        "        cv2.imwrite(f'/content/gdrive/MyDrive/AnimeFace/ColorUNet/Pred/{batch_idx}.png', output)\n",
        "        cv2.imwrite(f'/content/gdrive/MyDrive/AnimeFace/ColorUNet/X/{batch_idx}.png', images)\n",
        "        cv2.imwrite(f'/content/gdrive/MyDrive/AnimeFace/ColorUNet/Y/{batch_idx}.png', targets)"
      ],
      "metadata": {
        "id": "ctJ9js9SOmhi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}